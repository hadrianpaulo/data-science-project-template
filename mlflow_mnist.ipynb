{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init():\n",
    "    import mlflow\n",
    "    import uuid\n",
    "    mlflow.set_tracking_uri('http://localhost:5000/')\n",
    "    return mlflow.create_experiment(str(uuid.uuid4()))\n",
    "\n",
    "def load_data(experiment_id):\n",
    "    \"\"\"\n",
    "    Load the dataset from scikit-learn datasets module.\n",
    "    \n",
    "    Args:\n",
    "        experiment_id: MLFlow Experiment ID.\n",
    "    \n",
    "    Returns:\n",
    "        This returns a path of a persisted Pandas DataFrame of the MNIST dataset\n",
    "        which is available in the MLFlow server.\n",
    "    \"\"\"\n",
    "    from sklearn import datasets\n",
    "    import mlflow\n",
    "    with mlflow.start_run(experiment_id=experiment_id, nested=True):\n",
    "        X, y = datasets.load_digits(return_X_y=True, as_frame=True)\n",
    "        X['label'] = y\n",
    "        X.to_parquet('data/raw/mnist_raw.parquet')\n",
    "        mlflow.log_artifacts('data/raw')\n",
    "        return mlflow.search_runs(experiment_ids=[experiment_id])\n",
    "\n",
    "def preprocess_data(experiment_id, run_id, data_path, omit_digits=[], train_size=0.8):\n",
    "    \"\"\"\n",
    "    Preprocesses the provided MNIST data and excludes provided digits, if any. \n",
    "\n",
    "    Args:\n",
    "        experiment_id: MLFlow Experiment ID\n",
    "        omit_digits: a list of digits to be ommitted from the dataset.\n",
    "\n",
    "    Returns:\n",
    "        This returns a path of the training and testing DataFrames of the MNIST dataset\n",
    "        that may have some digits ommitted.\n",
    "    \"\"\"\n",
    "    import mlflow\n",
    "    import pandas as pd\n",
    "    from sklearn import model_selection\n",
    "    \n",
    "    with mlflow.start_run(experiment_id=experiment_id, run_id=run_id):\n",
    "        data = pd.read_parquet(data_path + '/mnist_raw.parquet')\n",
    "        # Remove duplicates in the input list of digits\n",
    "        omit_digits = set(omit_digits)\n",
    "\n",
    "        if train_size == 0:\n",
    "            return (None, data[~(data['label'].isin(omit_digits))])\n",
    "\n",
    "        train, test = model_selection.train_test_split(data[~(data['label'].isin(omit_digits))],\n",
    "                                                       train_size=train_size, random_state=42)\n",
    "        train.to_parquet('data/preprocessed/train.parquet')\n",
    "        test.to_parquet('data/preprocessed/test.parquet')\n",
    "        mlflow.log_artifacts('data/preprocessed')\n",
    "        return mlflow.search_runs(experiment_ids=[experiment_id])\n",
    "\n",
    "def create_features(experiment_id, run_id, data_path, is_train):\n",
    "    \"\"\"\n",
    "    Generates features for the provided dataset and returns the augmented dataset.\n",
    "    If the dataset provided is for training, a stateful feature generator will also be\n",
    "    returned.\n",
    "\n",
    "    Args:\n",
    "        experiment_id: MLFlow Experiment ID\n",
    "        data: a DataFrame containing the MNIST dataset.\n",
    "        is_train: a boolean that indicates whether the input data is a training set.\n",
    "                  This also indicates whether a feature_generator will be initialized or not.\n",
    "\n",
    "    Returns:\n",
    "        A path of the Pandas DataFrame containing the original and augmented datasets and an\n",
    "        optionally returned scikit-learn Pipeline of feature transformations.\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import joblib\n",
    "    from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "    from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "    from sklearn.decomposition import PCA, TruncatedSVD\n",
    "    \n",
    "    with mlflow.start_run(experiment_id=experiment_id, run_id=run_id):\n",
    "\n",
    "        if is_train:\n",
    "            data = pd.read_parquet(data_path + '/train.parquet')\n",
    "    \n",
    "            scaler = FeatureUnion([\n",
    "                ('standard_scaler', StandardScaler()),\n",
    "                ('robust_scaler', RobustScaler()),\n",
    "            ])\n",
    "            decomposer = FeatureUnion([\n",
    "                ('PCA', PCA(n_components=10, random_state=42)),\n",
    "                ('SVD', TruncatedSVD(random_state=42))\n",
    "            ])\n",
    "\n",
    "            feature_generator =  Pipeline([\n",
    "                ('scaler', scaler),\n",
    "                ('decomposer', decomposer)\n",
    "            ])\n",
    "\n",
    "            features = feature_generator.fit_transform(data.drop('label', axis=1))\n",
    "            joblib.dump(feature_generator, 'data/feature_generators/feature_generator')\n",
    "            filename = 'train'\n",
    "\n",
    "        else:\n",
    "            data = pd.read_parquet(data_path + '/preprocessed/test.parquet')\n",
    "            feature_generator = joblib.load(data_path + '/feature_generator')\n",
    "            features = feature_generator.transform(data.drop('label', axis=1))\n",
    "            filename = 'test'\n",
    "\n",
    "        augmented_data = np.concatenate([\n",
    "            data.drop('label', axis=1).values, \n",
    "            features,\n",
    "        ], axis=1)\n",
    "\n",
    "        augmented_data = pd.DataFrame(augmented_data)\n",
    "        augmented_data.columns = augmented_data.columns.astype(str)\n",
    "        augmented_data['label'] = data['label'].values\n",
    "        \n",
    "        augmented_data.to_parquet(f'data/processed/{filename}.parquet')\n",
    "        \n",
    "        mlflow.log_artifacts('data/feature_generators/')\n",
    "        mlflow.log_artifacts('data/processed/')\n",
    "        mlflow.log_artifacts('data')\n",
    "        return mlflow.search_runs(experiment_ids=[experiment_id])\n",
    "\n",
    "\n",
    "def generate_model(experiment_id, data_path, params, run_id=None):\n",
    "    \"\"\"\n",
    "    Train a model with the provided data, where cross-validation should ideally be\n",
    "    implemented.\n",
    "\n",
    "    Args:\n",
    "        experiment_id: MLFlow Experiment ID\n",
    "        data: a Pandas DataFrame where the model can generate training and validation sets.\n",
    "        params: model parameters\n",
    "    Returns:\n",
    "        A fully trained H2O model.\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    import mlflow\n",
    "    from mlflow import h2o as mlflow_h2o\n",
    "    from mlflow import sklearn as mlflow_sklearn\n",
    "    from sklearn import ensemble\n",
    "    import h2o\n",
    "    from h2o.automl import H2OAutoML\n",
    "    \n",
    "    data = pd.read_parquet(data_path + '/processed/train.parquet')\n",
    "    \n",
    "    with mlflow.start_run(experiment_id=experiment_id, run_id=run_id):\n",
    "        h2o.init()\n",
    "\n",
    "        features = data.columns.tolist()[:-1]\n",
    "        label = data.columns.tolist()[-1]\n",
    "        mlflow.log_params(params)\n",
    "        sklearn_model = ensemble.RandomForestClassifier(**params)\n",
    "        sklearn_model.fit(data[features], data[label])\n",
    "        \n",
    "        # Need to convert to H2O-compatible DataFrame\n",
    "        data = h2o.H2OFrame(data, column_names=data.columns.tolist())\n",
    "        data[label] = data[label].asfactor()\n",
    "        model = H2OAutoML(max_models=2, balance_classes=True, seed=params['random_state'])\n",
    "        model.train(features, label, data)\n",
    "        \n",
    "        lb = h2o.automl.get_leaderboard(model, extra_columns = 'ALL')\n",
    "        mlflow.log_metric(\"logloss\", lb.as_data_frame()['logloss'][0])\n",
    "        mlflow_h2o.log_model(model.leader, f\"model-{params}\")\n",
    "#         mlflow_sklearn.log_model(sklearn_model, f\"sklearn_model-{params}\")\n",
    "        mlflow.log_artifacts('data')\n",
    "        \n",
    "        return mlflow.search_runs(experiment_ids=[experiment_id])\n",
    "\n",
    "def generate_prediction(experiment_id, run_id, data_path, model):\n",
    "    \"\"\"\n",
    "    Generate predictions from a provided model and dataset.\n",
    "\n",
    "    Args:\n",
    "        experiment_id: MLFlow Experiment ID\n",
    "        data: a DataFrame where predictions shall be generated from.\n",
    "        model: an H2O model to generate predictions with.\n",
    "\n",
    "    Returns:\n",
    "        A NumPy Series of the generated predictions\n",
    "    \"\"\"\n",
    "    import h2o\n",
    "    import pandas as pd\n",
    "    \n",
    "    \n",
    "    \n",
    "    features = data.columns.tolist()[:-1]\n",
    "    label = data.columns.tolist()[-1]\n",
    "    # Need to convert to H2O-compatible DataFrame\n",
    "    data = h2o.H2OFrame(data, column_names=data.columns.tolist())\n",
    "    data[label] = data[label].asfactor()\n",
    "    preds = model.predict(data)['predict'].as_data_frame().values\n",
    "    \n",
    "    return preds\n",
    "\n",
    "def produce_reports(experiment_id, real, preds):\n",
    "    \"\"\"\n",
    "    Generate model performance reports, such as Precision, Recall, RMSE, LogLoss, etc.\n",
    "    \n",
    "    Args:\n",
    "        experiment_id: MLFlow Experiment ID\n",
    "        real: A NumPy array of the reference values.\n",
    "        preds: A NumPy array of the predicted values.\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    from sklearn.metrics import classification_report, matthews_corrcoef\n",
    "    \n",
    "    print(f'Matthews Correlation Coefficient: {matthews_corrcoef(real, preds)}')\n",
    "    print(\n",
    "        classification_report(real, preds, digits=4)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "experiment_id = init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_path = load_data(experiment_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_path = preprocess_data(experiment_id=experiment_id, \n",
    "                                    run_id=raw_path['run_id'][0], \n",
    "                                    data_path=raw_path['artifact_uri'][0], omit_digits=[1, 7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_train_path = create_features(experiment_id=experiment_id, \n",
    "                                       run_id=raw_path['run_id'][0], \n",
    "                                       data_path=raw_path['artifact_uri'][0],\n",
    "                                       is_train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_test_path = create_features(\n",
    "                                    experiment_id=experiment_id, \n",
    "                                    run_id=raw_path['run_id'][0], \n",
    "                                    data_path=raw_path['artifact_uri'][0],\n",
    "                                    is_train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking whether there is an H2O instance running at http://localhost:54321 ..... not found.\n",
      "Attempting to start a local H2O server...\n",
      "  Java Version: openjdk version \"1.8.0_152-release\"; OpenJDK Runtime Environment (build 1.8.0_152-release-1056-b12); OpenJDK 64-Bit Server VM (build 25.152-b12, mixed mode)\n",
      "  Starting server from /home/hadrian/anaconda3/envs/py36/lib/python3.6/site-packages/h2o/backend/bin/h2o.jar\n",
      "  Ice root: /tmp/tmpaeqqv7xj\n",
      "  JVM stdout: /tmp/tmpaeqqv7xj/h2o_hadrian_started_from_python.out\n",
      "  JVM stderr: /tmp/tmpaeqqv7xj/h2o_hadrian_started_from_python.err\n",
      "  Server is running at http://127.0.0.1:54321\n",
      "Connecting to H2O server at http://127.0.0.1:54321 ... successful.\n",
      "Warning: Your H2O cluster version is too old (4 months and 12 days)! Please download and install the latest version from http://h2o.ai/download/\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td>H2O cluster uptime:</td>\n",
       "<td>01 secs</td></tr>\n",
       "<tr><td>H2O cluster timezone:</td>\n",
       "<td>Asia/Manila</td></tr>\n",
       "<tr><td>H2O data parsing timezone:</td>\n",
       "<td>UTC</td></tr>\n",
       "<tr><td>H2O cluster version:</td>\n",
       "<td>3.28.1.2</td></tr>\n",
       "<tr><td>H2O cluster version age:</td>\n",
       "<td>4 months and 12 days !!!</td></tr>\n",
       "<tr><td>H2O cluster name:</td>\n",
       "<td>H2O_from_python_hadrian_lk2zjv</td></tr>\n",
       "<tr><td>H2O cluster total nodes:</td>\n",
       "<td>1</td></tr>\n",
       "<tr><td>H2O cluster free memory:</td>\n",
       "<td>1.672 Gb</td></tr>\n",
       "<tr><td>H2O cluster total cores:</td>\n",
       "<td>8</td></tr>\n",
       "<tr><td>H2O cluster allowed cores:</td>\n",
       "<td>8</td></tr>\n",
       "<tr><td>H2O cluster status:</td>\n",
       "<td>accepting new members, healthy</td></tr>\n",
       "<tr><td>H2O connection url:</td>\n",
       "<td>http://127.0.0.1:54321</td></tr>\n",
       "<tr><td>H2O connection proxy:</td>\n",
       "<td>{'http': None, 'https': None}</td></tr>\n",
       "<tr><td>H2O internal security:</td>\n",
       "<td>False</td></tr>\n",
       "<tr><td>H2O API Extensions:</td>\n",
       "<td>Amazon S3, XGBoost, Algos, AutoML, Core V3, TargetEncoder, Core V4</td></tr>\n",
       "<tr><td>Python version:</td>\n",
       "<td>3.6.10 final</td></tr></table></div>"
      ],
      "text/plain": [
       "--------------------------  ------------------------------------------------------------------\n",
       "H2O cluster uptime:         01 secs\n",
       "H2O cluster timezone:       Asia/Manila\n",
       "H2O data parsing timezone:  UTC\n",
       "H2O cluster version:        3.28.1.2\n",
       "H2O cluster version age:    4 months and 12 days !!!\n",
       "H2O cluster name:           H2O_from_python_hadrian_lk2zjv\n",
       "H2O cluster total nodes:    1\n",
       "H2O cluster free memory:    1.672 Gb\n",
       "H2O cluster total cores:    8\n",
       "H2O cluster allowed cores:  8\n",
       "H2O cluster status:         accepting new members, healthy\n",
       "H2O connection url:         http://127.0.0.1:54321\n",
       "H2O connection proxy:       {'http': None, 'https': None}\n",
       "H2O internal security:      False\n",
       "H2O API Extensions:         Amazon S3, XGBoost, Algos, AutoML, Core V3, TargetEncoder, Core V4\n",
       "Python version:             3.6.10 final\n",
       "--------------------------  ------------------------------------------------------------------"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parse progress: |█████████████████████████████████████████████████████████| 100%\n",
      "AutoML progress: |████████████████████████████████████████████████████████| 100%\n"
     ]
    }
   ],
   "source": [
    "model_path = generate_model(experiment_id=experiment_id, \n",
    "                                    run_id=raw_path['run_id'][0], \n",
    "                                    data_path=raw_path['artifact_uri'][0], params={'random_state': 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking whether there is an H2O instance running at http://localhost:54321 . connected.\n",
      "Warning: Your H2O cluster version is too old (4 months and 12 days)! Please download and install the latest version from http://h2o.ai/download/\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td>H2O cluster uptime:</td>\n",
       "<td>40 secs</td></tr>\n",
       "<tr><td>H2O cluster timezone:</td>\n",
       "<td>Asia/Manila</td></tr>\n",
       "<tr><td>H2O data parsing timezone:</td>\n",
       "<td>UTC</td></tr>\n",
       "<tr><td>H2O cluster version:</td>\n",
       "<td>3.28.1.2</td></tr>\n",
       "<tr><td>H2O cluster version age:</td>\n",
       "<td>4 months and 12 days !!!</td></tr>\n",
       "<tr><td>H2O cluster name:</td>\n",
       "<td>H2O_from_python_hadrian_lk2zjv</td></tr>\n",
       "<tr><td>H2O cluster total nodes:</td>\n",
       "<td>1</td></tr>\n",
       "<tr><td>H2O cluster free memory:</td>\n",
       "<td>1.339 Gb</td></tr>\n",
       "<tr><td>H2O cluster total cores:</td>\n",
       "<td>8</td></tr>\n",
       "<tr><td>H2O cluster allowed cores:</td>\n",
       "<td>8</td></tr>\n",
       "<tr><td>H2O cluster status:</td>\n",
       "<td>locked, healthy</td></tr>\n",
       "<tr><td>H2O connection url:</td>\n",
       "<td>http://localhost:54321</td></tr>\n",
       "<tr><td>H2O connection proxy:</td>\n",
       "<td>{'http': None, 'https': None}</td></tr>\n",
       "<tr><td>H2O internal security:</td>\n",
       "<td>False</td></tr>\n",
       "<tr><td>H2O API Extensions:</td>\n",
       "<td>Amazon S3, XGBoost, Algos, AutoML, Core V3, TargetEncoder, Core V4</td></tr>\n",
       "<tr><td>Python version:</td>\n",
       "<td>3.6.10 final</td></tr></table></div>"
      ],
      "text/plain": [
       "--------------------------  ------------------------------------------------------------------\n",
       "H2O cluster uptime:         40 secs\n",
       "H2O cluster timezone:       Asia/Manila\n",
       "H2O data parsing timezone:  UTC\n",
       "H2O cluster version:        3.28.1.2\n",
       "H2O cluster version age:    4 months and 12 days !!!\n",
       "H2O cluster name:           H2O_from_python_hadrian_lk2zjv\n",
       "H2O cluster total nodes:    1\n",
       "H2O cluster free memory:    1.339 Gb\n",
       "H2O cluster total cores:    8\n",
       "H2O cluster allowed cores:  8\n",
       "H2O cluster status:         locked, healthy\n",
       "H2O connection url:         http://localhost:54321\n",
       "H2O connection proxy:       {'http': None, 'https': None}\n",
       "H2O internal security:      False\n",
       "H2O API Extensions:         Amazon S3, XGBoost, Algos, AutoML, Core V3, TargetEncoder, Core V4\n",
       "Python version:             3.6.10 final\n",
       "--------------------------  ------------------------------------------------------------------"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parse progress: |█████████████████████████████████████████████████████████| 100%\n",
      "AutoML progress: |████████████████████████████████████████████████████████| 100%\n"
     ]
    }
   ],
   "source": [
    "model_path = generate_model(experiment_id=experiment_id, \n",
    "                                    data_path=raw_path['artifact_uri'][0], params={'random_state': 42})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
