{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    \"\"\"\n",
    "    Load the dataset from scikit-learn datasets module.\n",
    "\n",
    "    Returns:\n",
    "        This returns a Pandas DataFrame of the MNIST dataset\n",
    "    \"\"\"\n",
    "    from sklearn import datasets\n",
    "    \n",
    "    X, y = datasets.load_digits(return_X_y=True, as_frame=True)\n",
    "    X['label'] = y\n",
    "    return X\n",
    "\n",
    "\n",
    "def preprocess_data(data, omit_digits=[], train_size=0.8):\n",
    "    \"\"\"\n",
    "    Preprocesses the provided MNIST data and excludes provided digits, if any. \n",
    "\n",
    "    Args:\n",
    "        data: the input DataFrame to be preprocessed.\n",
    "        omit_digits: a list of digits to be ommitted from the dataset. \n",
    "\n",
    "    Returns:\n",
    "        This returns a tuple of training and testing DataFrames of the MNIST dataset\n",
    "        that may have some digits ommitted.\n",
    "    \"\"\"\n",
    "    from sklearn import model_selection\n",
    "    \n",
    "    # Remove duplicates in the input list of digits\n",
    "    omit_digits = set(omit_digits)\n",
    "    \n",
    "    if train_size == 0:\n",
    "        return (None, data[~(data['label'].isin(omit_digits))])\n",
    "    \n",
    "    train, test = model_selection.train_test_split(data[~(data['label'].isin(omit_digits))],\n",
    "                                                   train_size=train_size, random_state=42)\n",
    "    return (train, test)\n",
    "\n",
    "def create_features(data, is_train, feature_generator=None):\n",
    "    \"\"\"\n",
    "    Generates features for the provided dataset and returns the augmented dataset.\n",
    "    If the dataset provided is for training, a stateful feature generator will also be\n",
    "    returned.\n",
    "\n",
    "    Args:\n",
    "        data: a DataFrame containing the MNIST dataset.\n",
    "        is_train: a boolean that indicates whether the input data is a training set.\n",
    "                  This also indicates whether a feature_generator will be initialized or not.\n",
    "        feature_generator: This is a fitted scikit-learn Pipeline that contains feature transformations\n",
    "                            generated from a training set.\n",
    "\n",
    "    Returns:\n",
    "        augmented_data: the Pandas DataFrame containing the original and augmented datasets.\n",
    "        feature_generator: Optionally returned scikit-learn Pipeline of feature transformations.\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "    from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "    from sklearn.decomposition import PCA, TruncatedSVD\n",
    "    \n",
    "    if not is_train and feature_generator is None:\n",
    "        raise ValueError('A test set should have a feature_generator provided.')\n",
    "        \n",
    "    if is_train:\n",
    "        scaler = FeatureUnion([\n",
    "            ('standard_scaler', StandardScaler()),\n",
    "            ('robust_scaler', RobustScaler()),\n",
    "        ])\n",
    "        decomposer = FeatureUnion([\n",
    "            ('PCA', PCA(n_components=10, random_state=42)),\n",
    "            ('SVD', TruncatedSVD(random_state=42))\n",
    "        ])\n",
    "        \n",
    "        feature_generator =  Pipeline([\n",
    "            ('scaler', scaler),\n",
    "            ('decomposer', decomposer)\n",
    "        ])\n",
    "        \n",
    "        features = feature_generator.fit_transform(data.drop('label', axis=1))\n",
    "    \n",
    "    else:\n",
    "        features = feature_generator.transform(data.drop('label', axis=1))\n",
    "        \n",
    "    augmented_data = np.concatenate([\n",
    "        data.drop('label', axis=1).values, \n",
    "        features,\n",
    "    ], axis=1)\n",
    "    \n",
    "    augmented_data = pd.DataFrame(augmented_data)\n",
    "    augmented_data.columns = augmented_data.columns.astype(str)\n",
    "    augmented_data['label'] = data['label'].values\n",
    "    \n",
    "    return (augmented_data, feature_generator)\n",
    "\n",
    "\n",
    "def generate_model(data):\n",
    "    \"\"\"\n",
    "    Train a model with the provided data, where cross-validation should ideally be\n",
    "    implemented.\n",
    "\n",
    "    Args:\n",
    "        data: a Pandas DataFrame where the model can generate training and validation sets.\n",
    "\n",
    "    Returns:\n",
    "        A fully trained H2O model.\n",
    "    \"\"\"\n",
    "    \n",
    "    import h2o\n",
    "    from h2o.automl import H2OAutoML\n",
    "    \n",
    "    h2o.init()\n",
    "    \n",
    "    features = data.columns.tolist()[:-1]\n",
    "    label = data.columns.tolist()[-1]\n",
    "    # Need to convert to H2O-compatible DataFrame\n",
    "    data = h2o.H2OFrame(data, column_names=data.columns.tolist())\n",
    "    data[label] = data[label].asfactor()\n",
    "    model = H2OAutoML(max_models=3, balance_classes=True, seed=42)\n",
    "    model.train(features, label, data)\n",
    "    \n",
    "    return model\n",
    "\n",
    "def generate_prediction(data, model):\n",
    "    \"\"\"\n",
    "    Generate predictions from a provided model and dataset.\n",
    "\n",
    "    Args:\n",
    "        data: a DataFrame where predictions shall be generated from.\n",
    "        model: an H2O model to generate predictions with.\n",
    "\n",
    "    Returns:\n",
    "        A NumPy Series of the generated predictions\n",
    "    \"\"\"\n",
    "    import h2o\n",
    "    \n",
    "    features = data.columns.tolist()[:-1]\n",
    "    label = data.columns.tolist()[-1]\n",
    "    # Need to convert to H2O-compatible DataFrame\n",
    "    data = h2o.H2OFrame(data, column_names=data.columns.tolist())\n",
    "    data[label] = data[label].asfactor()\n",
    "    preds = model.predict(data)['predict'].as_data_frame().values\n",
    "    \n",
    "    return preds\n",
    "\n",
    "def produce_reports(real, preds):\n",
    "    \"\"\"\n",
    "    Generate model performance reports, such as Precision, Recall, RMSE, LogLoss, etc.\n",
    "    \n",
    "    Args:\n",
    "        real: A NumPy array of the reference values.\n",
    "        preds: A NumPy array of the predicted values.\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    from sklearn.metrics import classification_report, matthews_corrcoef\n",
    "    \n",
    "    print(f'Matthews Correlation Coefficient: {matthews_corrcoef(real, preds)}')\n",
    "    print(\n",
    "        classification_report(real, preds, digits=4)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_raw = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = preprocess_data(data_raw, omit_digits=[8, 9], train_size=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_augmented, feature_generator = create_features(train, is_train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_augmented, _ = create_features(test, is_train=False, feature_generator=feature_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking whether there is an H2O instance running at http://localhost:54321 ..... not found.\n",
      "Attempting to start a local H2O server...\n",
      "  Java Version: openjdk version \"11.0.1\" 2018-10-16 LTS; OpenJDK Runtime Environment Zulu11.2+3 (build 11.0.1+13-LTS); OpenJDK 64-Bit Server VM Zulu11.2+3 (build 11.0.1+13-LTS, mixed mode)\n",
      "  Starting server from /home/hadrian/anaconda3/envs/py36/lib/python3.6/site-packages/h2o/backend/bin/h2o.jar\n",
      "  Ice root: /tmp/tmpcrg01srl\n",
      "  JVM stdout: /tmp/tmpcrg01srl/h2o_hadrian_started_from_python.out\n",
      "  JVM stderr: /tmp/tmpcrg01srl/h2o_hadrian_started_from_python.err\n",
      "  Server is running at http://127.0.0.1:54321\n",
      "Connecting to H2O server at http://127.0.0.1:54321 ... successful.\n",
      "Warning: Your H2O cluster version is too old (4 months and 10 days)! Please download and install the latest version from http://h2o.ai/download/\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td>H2O cluster uptime:</td>\n",
       "<td>01 secs</td></tr>\n",
       "<tr><td>H2O cluster timezone:</td>\n",
       "<td>Asia/Manila</td></tr>\n",
       "<tr><td>H2O data parsing timezone:</td>\n",
       "<td>UTC</td></tr>\n",
       "<tr><td>H2O cluster version:</td>\n",
       "<td>3.28.1.2</td></tr>\n",
       "<tr><td>H2O cluster version age:</td>\n",
       "<td>4 months and 10 days !!!</td></tr>\n",
       "<tr><td>H2O cluster name:</td>\n",
       "<td>H2O_from_python_hadrian_omgcbv</td></tr>\n",
       "<tr><td>H2O cluster total nodes:</td>\n",
       "<td>1</td></tr>\n",
       "<tr><td>H2O cluster free memory:</td>\n",
       "<td>1.881 Gb</td></tr>\n",
       "<tr><td>H2O cluster total cores:</td>\n",
       "<td>8</td></tr>\n",
       "<tr><td>H2O cluster allowed cores:</td>\n",
       "<td>8</td></tr>\n",
       "<tr><td>H2O cluster status:</td>\n",
       "<td>accepting new members, healthy</td></tr>\n",
       "<tr><td>H2O connection url:</td>\n",
       "<td>http://127.0.0.1:54321</td></tr>\n",
       "<tr><td>H2O connection proxy:</td>\n",
       "<td>{'http': None, 'https': None}</td></tr>\n",
       "<tr><td>H2O internal security:</td>\n",
       "<td>False</td></tr>\n",
       "<tr><td>H2O API Extensions:</td>\n",
       "<td>Amazon S3, XGBoost, Algos, AutoML, Core V3, TargetEncoder, Core V4</td></tr>\n",
       "<tr><td>Python version:</td>\n",
       "<td>3.6.10 final</td></tr></table></div>"
      ],
      "text/plain": [
       "--------------------------  ------------------------------------------------------------------\n",
       "H2O cluster uptime:         01 secs\n",
       "H2O cluster timezone:       Asia/Manila\n",
       "H2O data parsing timezone:  UTC\n",
       "H2O cluster version:        3.28.1.2\n",
       "H2O cluster version age:    4 months and 10 days !!!\n",
       "H2O cluster name:           H2O_from_python_hadrian_omgcbv\n",
       "H2O cluster total nodes:    1\n",
       "H2O cluster free memory:    1.881 Gb\n",
       "H2O cluster total cores:    8\n",
       "H2O cluster allowed cores:  8\n",
       "H2O cluster status:         accepting new members, healthy\n",
       "H2O connection url:         http://127.0.0.1:54321\n",
       "H2O connection proxy:       {'http': None, 'https': None}\n",
       "H2O internal security:      False\n",
       "H2O API Extensions:         Amazon S3, XGBoost, Algos, AutoML, Core V3, TargetEncoder, Core V4\n",
       "Python version:             3.6.10 final\n",
       "--------------------------  ------------------------------------------------------------------"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parse progress: |█████████████████████████████████████████████████████████| 100%\n",
      "AutoML progress: |████████████████████████████████████████████████████████| 100%\n"
     ]
    }
   ],
   "source": [
    "model = generate_model(train_augmented)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parse progress: |█████████████████████████████████████████████████████████| 100%\n",
      "xgboost prediction progress: |████████████████████████████████████████████| 100%\n"
     ]
    }
   ],
   "source": [
    "preds = generate_prediction(test_augmented, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matthews Correlation Coefficient: 0.9684231781600441\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9767    0.9767    0.9767        43\n",
      "           1     0.9737    1.0000    0.9867        37\n",
      "           2     1.0000    1.0000    1.0000        23\n",
      "           3     1.0000    0.9474    0.9730        38\n",
      "           4     0.8913    0.9762    0.9318        42\n",
      "           5     1.0000    1.0000    1.0000        30\n",
      "           6     1.0000    0.9189    0.9577        37\n",
      "           7     0.9744    0.9744    0.9744        39\n",
      "\n",
      "    accuracy                         0.9723       289\n",
      "   macro avg     0.9770    0.9742    0.9750       289\n",
      "weighted avg     0.9739    0.9723    0.9725       289\n",
      "\n"
     ]
    }
   ],
   "source": [
    "produce_reports(test['label'], preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finalizing for Prototype\n",
    "This can be easily parallelized and exected in batches or on-the-fly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_raw = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, data_preprocessed = preprocess_data(data_raw, train_size=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augmented, _ = create_features(data_preprocessed, is_train=False, feature_generator=feature_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parse progress: |█████████████████████████████████████████████████████████| 100%\n",
      "xgboost prediction progress: |████████████████████████████████████████████| 100%\n"
     ]
    }
   ],
   "source": [
    "preds = generate_prediction(data_augmented, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matthews Correlation Coefficient: 0.7894216635256807\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hadrian/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9077    0.9944    0.9491       178\n",
      "           1     0.7879    1.0000    0.8814       182\n",
      "           2     0.9267    1.0000    0.9620       177\n",
      "           3     0.5171    0.9891    0.6792       183\n",
      "           4     0.8911    0.9945    0.9399       181\n",
      "           5     0.7712    1.0000    0.8708       182\n",
      "           6     0.9622    0.9834    0.9727       181\n",
      "           7     0.8599    0.9944    0.9223       179\n",
      "           8     0.0000    0.0000    0.0000       174\n",
      "           9     0.0000    0.0000    0.0000       180\n",
      "\n",
      "    accuracy                         0.7986      1797\n",
      "   macro avg     0.6624    0.7956    0.7177      1797\n",
      "weighted avg     0.6641    0.7986    0.7199      1797\n",
      "\n"
     ]
    }
   ],
   "source": [
    "produce_reports(data_augmented['label'], preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
