{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    \"\"\"\n",
    "    Load the dataset from scikit-learn datasets module.\n",
    "\n",
    "    Returns:\n",
    "        This returns a Pandas DataFrame of the MNIST dataset\n",
    "    \"\"\"\n",
    "    from sklearn import datasets\n",
    "    \n",
    "    X, y = datasets.load_digits(return_X_y=True, as_frame=True)\n",
    "    X['label'] = y\n",
    "    return X\n",
    "\n",
    "\n",
    "def preprocess_data(data, omit_digits=[], train_size=0.8, delete_data=0.1):\n",
    "    \"\"\"\n",
    "    Preprocesses the provided MNIST data and excludes provided digits, if any. \n",
    "\n",
    "    Args:\n",
    "        data: the input DataFrame to be preprocessed.\n",
    "        omit_digits: a list of digits to be ommitted from the dataset. \n",
    "        delete_data: How much of a fraction of the DataFrame to replace values with np.NaN (0,1].\n",
    "    Returns:\n",
    "        This returns a tuple of training and testing DataFrames of the MNIST dataset\n",
    "        that may have some digits ommitted.\n",
    "    \"\"\"\n",
    "    import random\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from sklearn import model_selection\n",
    "    \n",
    "    # Remove duplicates in the input list of digits\n",
    "    omit_digits = set(omit_digits)\n",
    "    \n",
    "    if train_size == 0:\n",
    "        return (None, data[~(data['label'].isin(omit_digits))])\n",
    "    \n",
    "    train, test = model_selection.train_test_split(data[~(data['label'].isin(omit_digits))],\n",
    "                                                   train_size=train_size, random_state=42)\n",
    "    \n",
    "    train = pd.DataFrame(train, columns=data.columns)\n",
    "    test = pd.DataFrame(test, columns=data.columns)\n",
    "    \n",
    "    train = create_missing_data(train, delete_data, skip_cols=['label'])\n",
    "    test = create_missing_data(test, delete_data, skip_cols=['label'])\n",
    "            \n",
    "    return (train, test)\n",
    "\n",
    "def create_features(data, is_train, feature_generator=None):\n",
    "    \"\"\"\n",
    "    Generates features for the provided dataset and returns the augmented dataset.\n",
    "    If the dataset provided is for training, a stateful feature generator will also be\n",
    "    returned.\n",
    "\n",
    "    Args:\n",
    "        data: a DataFrame containing the MNIST dataset.\n",
    "        is_train: a boolean that indicates whether the input data is a training set.\n",
    "                  This also indicates whether a feature_generator will be initialized or not.\n",
    "        feature_generator: This is a fitted scikit-learn Pipeline that contains feature transformations\n",
    "                            generated from a training set.\n",
    "\n",
    "    Returns:\n",
    "        augmented_data: the Pandas DataFrame containing the original and augmented datasets.\n",
    "        feature_generator: Optionally returned scikit-learn Pipeline of feature transformations.\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "    from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "    from sklearn.decomposition import PCA, TruncatedSVD\n",
    "    from sklearn.experimental import enable_iterative_imputer  # noqa\n",
    "    from sklearn.impute import IterativeImputer, SimpleImputer, KNNImputer\n",
    "    from sklearn.linear_model import LassoLars\n",
    "    \n",
    "    if not is_train and feature_generator is None:\n",
    "        raise ValueError('A test set should have a feature_generator provided.')\n",
    "        \n",
    "    if is_train:\n",
    "        scaler = FeatureUnion([\n",
    "            ('standard_scaler', StandardScaler()),\n",
    "            ('robust_scaler', RobustScaler()),\n",
    "        ])\n",
    "        imputers = FeatureUnion([\n",
    "            ('Mean Imputer', SimpleImputer(strategy='mean')),\n",
    "            ('LassoLars Regression Imputer', IterativeImputer(\n",
    "                LassoLars(random_state=42),\n",
    "                random_state=42)),\n",
    "            ('KNN Imputer', KNNImputer())\n",
    "        ])\n",
    "        decomposer = FeatureUnion([\n",
    "            ('PCA', PCA(n_components=10, random_state=42)),\n",
    "            ('SVD', TruncatedSVD(random_state=42))\n",
    "        ])\n",
    "        \n",
    "        feature_generator =  Pipeline([\n",
    "            ('impute', imputers),\n",
    "            ('scaler', scaler),\n",
    "            ('decomposer', decomposer)\n",
    "        ])\n",
    "        \n",
    "        features = feature_generator.fit_transform(data.drop('label', axis=1))\n",
    "    \n",
    "    else:\n",
    "        features = feature_generator.transform(data.drop('label', axis=1))\n",
    "        \n",
    "    augmented_data = np.concatenate([\n",
    "        data.drop('label', axis=1).values, \n",
    "        features,\n",
    "    ], axis=1)\n",
    "    \n",
    "    augmented_data = pd.DataFrame(augmented_data)\n",
    "    augmented_data.columns = augmented_data.columns.astype(str)\n",
    "    augmented_data['label'] = data['label'].values\n",
    "    \n",
    "    return (augmented_data, feature_generator)\n",
    "\n",
    "\n",
    "def generate_model(data):\n",
    "    \"\"\"\n",
    "    Train a model with the provided data, where cross-validation should ideally be\n",
    "    implemented.\n",
    "\n",
    "    Args:\n",
    "        data: a Pandas DataFrame where the model can generate training and validation sets.\n",
    "\n",
    "    Returns:\n",
    "        A fully trained H2O model.\n",
    "    \"\"\"\n",
    "    \n",
    "    import h2o\n",
    "    from h2o.automl import H2OAutoML\n",
    "    \n",
    "    h2o.init()\n",
    "    \n",
    "    features = data.columns.tolist()[:-1]\n",
    "    label = data.columns.tolist()[-1]\n",
    "    # Need to convert to H2O-compatible DataFrame\n",
    "    data = h2o.H2OFrame(data, column_names=data.columns.tolist())\n",
    "    data[label] = data[label].asfactor()\n",
    "    model = H2OAutoML(max_models=3, balance_classes=True, seed=42)\n",
    "    model.train(features, label, data)\n",
    "    \n",
    "    return model\n",
    "\n",
    "def generate_prediction(data, model):\n",
    "    \"\"\"\n",
    "    Generate predictions from a provided model and dataset.\n",
    "\n",
    "    Args:\n",
    "        data: a DataFrame where predictions shall be generated from.\n",
    "        model: an H2O model to generate predictions with.\n",
    "\n",
    "    Returns:\n",
    "        A NumPy Series of the generated predictions\n",
    "    \"\"\"\n",
    "    import h2o\n",
    "    \n",
    "    features = data.columns.tolist()[:-1]\n",
    "    label = data.columns.tolist()[-1]\n",
    "    # Need to convert to H2O-compatible DataFrame\n",
    "    data = h2o.H2OFrame(data, column_names=data.columns.tolist())\n",
    "    data[label] = data[label].asfactor()\n",
    "    preds = model.predict(data)['predict'].as_data_frame().values\n",
    "    \n",
    "    return preds\n",
    "\n",
    "def produce_reports(real, preds):\n",
    "    \"\"\"\n",
    "    Generate model performance reports, such as Precision, Recall, RMSE, LogLoss, etc.\n",
    "    \n",
    "    Args:\n",
    "        real: A NumPy array of the reference values.\n",
    "        preds: A NumPy array of the predicted values.\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    from sklearn.metrics import classification_report, matthews_corrcoef\n",
    "    \n",
    "    print(f'Matthews Correlation Coefficient: {matthews_corrcoef(real, preds)}')\n",
    "    print(\n",
    "        classification_report(real, preds, digits=4)\n",
    "    )\n",
    "    \n",
    "def create_missing_data(df, delete_data, skip_cols=[]):\n",
    "    import random\n",
    "    import numpy as np\n",
    "    \n",
    "    df = df.copy()\n",
    "    df_keep = df[skip_cols].copy()\n",
    "    df = df[df.columns[~df.columns.isin(skip_cols)]]\n",
    "    \n",
    "    ix = [(row, col) for row in range(df.shape[0]) for col in range(df.shape[1])]\n",
    "    for row, col in random.sample(ix, int(round(delete_data*len(ix)))):\n",
    "        df.iat[row, col] = np.nan\n",
    "    \n",
    "    return df.merge(df_keep, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_raw = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = preprocess_data(data_raw, omit_digits=[2, 8], train_size=0.8, delete_data=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_augmented, feature_generator = create_features(train, is_train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_augmented, _ = create_features(test, is_train=False, feature_generator=feature_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking whether there is an H2O instance running at http://localhost:54321 ..... not found.\n",
      "Attempting to start a local H2O server...\n",
      "  Java Version: openjdk version \"1.8.0_152-release\"; OpenJDK Runtime Environment (build 1.8.0_152-release-1056-b12); OpenJDK 64-Bit Server VM (build 25.152-b12, mixed mode)\n",
      "  Starting server from /home/hadrian/anaconda3/envs/py36/lib/python3.6/site-packages/h2o/backend/bin/h2o.jar\n",
      "  Ice root: /tmp/tmpl10728sg\n",
      "  JVM stdout: /tmp/tmpl10728sg/h2o_hadrian_started_from_python.out\n",
      "  JVM stderr: /tmp/tmpl10728sg/h2o_hadrian_started_from_python.err\n",
      "  Server is running at http://127.0.0.1:54321\n",
      "Connecting to H2O server at http://127.0.0.1:54321 ... successful.\n",
      "Warning: Your H2O cluster version is too old (4 months and 18 days)! Please download and install the latest version from http://h2o.ai/download/\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td>H2O cluster uptime:</td>\n",
       "<td>01 secs</td></tr>\n",
       "<tr><td>H2O cluster timezone:</td>\n",
       "<td>Asia/Manila</td></tr>\n",
       "<tr><td>H2O data parsing timezone:</td>\n",
       "<td>UTC</td></tr>\n",
       "<tr><td>H2O cluster version:</td>\n",
       "<td>3.28.1.2</td></tr>\n",
       "<tr><td>H2O cluster version age:</td>\n",
       "<td>4 months and 18 days !!!</td></tr>\n",
       "<tr><td>H2O cluster name:</td>\n",
       "<td>H2O_from_python_hadrian_e699uf</td></tr>\n",
       "<tr><td>H2O cluster total nodes:</td>\n",
       "<td>1</td></tr>\n",
       "<tr><td>H2O cluster free memory:</td>\n",
       "<td>1.672 Gb</td></tr>\n",
       "<tr><td>H2O cluster total cores:</td>\n",
       "<td>8</td></tr>\n",
       "<tr><td>H2O cluster allowed cores:</td>\n",
       "<td>8</td></tr>\n",
       "<tr><td>H2O cluster status:</td>\n",
       "<td>accepting new members, healthy</td></tr>\n",
       "<tr><td>H2O connection url:</td>\n",
       "<td>http://127.0.0.1:54321</td></tr>\n",
       "<tr><td>H2O connection proxy:</td>\n",
       "<td>{'http': None, 'https': None}</td></tr>\n",
       "<tr><td>H2O internal security:</td>\n",
       "<td>False</td></tr>\n",
       "<tr><td>H2O API Extensions:</td>\n",
       "<td>Amazon S3, XGBoost, Algos, AutoML, Core V3, TargetEncoder, Core V4</td></tr>\n",
       "<tr><td>Python version:</td>\n",
       "<td>3.6.10 final</td></tr></table></div>"
      ],
      "text/plain": [
       "--------------------------  ------------------------------------------------------------------\n",
       "H2O cluster uptime:         01 secs\n",
       "H2O cluster timezone:       Asia/Manila\n",
       "H2O data parsing timezone:  UTC\n",
       "H2O cluster version:        3.28.1.2\n",
       "H2O cluster version age:    4 months and 18 days !!!\n",
       "H2O cluster name:           H2O_from_python_hadrian_e699uf\n",
       "H2O cluster total nodes:    1\n",
       "H2O cluster free memory:    1.672 Gb\n",
       "H2O cluster total cores:    8\n",
       "H2O cluster allowed cores:  8\n",
       "H2O cluster status:         accepting new members, healthy\n",
       "H2O connection url:         http://127.0.0.1:54321\n",
       "H2O connection proxy:       {'http': None, 'https': None}\n",
       "H2O internal security:      False\n",
       "H2O API Extensions:         Amazon S3, XGBoost, Algos, AutoML, Core V3, TargetEncoder, Core V4\n",
       "Python version:             3.6.10 final\n",
       "--------------------------  ------------------------------------------------------------------"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parse progress: |█████████████████████████████████████████████████████████| 100%\n",
      "AutoML progress: |████████████████████████████████████████████████████████| 100%\n"
     ]
    }
   ],
   "source": [
    "model = generate_model(train_augmented)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parse progress: |█████████████████████████████████████████████████████████| 100%\n",
      "xgboost prediction progress: |████████████████████████████████████████████| 100%\n"
     ]
    }
   ],
   "source": [
    "preds = generate_prediction(test_augmented, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matthews Correlation Coefficient: 0.9411075284537671\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    1.0000    1.0000        45\n",
      "           1     0.8684    1.0000    0.9296        33\n",
      "           3     0.9500    0.9048    0.9268        42\n",
      "           4     0.9722    0.9459    0.9589        37\n",
      "           5     0.9722    0.9459    0.9589        37\n",
      "           6     1.0000    0.8966    0.9455        29\n",
      "           7     0.8750    1.0000    0.9333        28\n",
      "           9     0.9459    0.8974    0.9211        39\n",
      "\n",
      "    accuracy                         0.9483       290\n",
      "   macro avg     0.9480    0.9488    0.9468       290\n",
      "weighted avg     0.9514    0.9483    0.9484       290\n",
      "\n"
     ]
    }
   ],
   "source": [
    "produce_reports(test['label'], preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finalizing for Prototype\n",
    "This can be easily parallelized and exected in batches or on-the-fly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_raw = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, data_preprocessed = preprocess_data(data_raw, train_size=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augmented, _ = create_features(data_preprocessed, is_train=False, feature_generator=feature_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parse progress: |█████████████████████████████████████████████████████████| 100%\n",
      "xgboost prediction progress: |████████████████████████████████████████████| 100%\n"
     ]
    }
   ],
   "source": [
    "preds = generate_prediction(data_augmented, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matthews Correlation Coefficient: 0.78434739370246\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hadrian/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9888    0.9944    0.9916       178\n",
      "           1     0.7358    0.9945    0.8458       182\n",
      "           2     0.0000    0.0000    0.0000       177\n",
      "           3     0.5144    0.9781    0.6742       183\n",
      "           4     0.9521    0.9890    0.9702       181\n",
      "           5     0.8249    0.9835    0.8972       182\n",
      "           6     0.9418    0.9834    0.9622       181\n",
      "           7     0.9227    1.0000    0.9598       179\n",
      "           8     0.0000    0.0000    0.0000       174\n",
      "           9     0.7415    0.9722    0.8413       180\n",
      "\n",
      "    accuracy                         0.7941      1797\n",
      "   macro avg     0.6622    0.7895    0.7142      1797\n",
      "weighted avg     0.6653    0.7941    0.7179      1797\n",
      "\n"
     ]
    }
   ],
   "source": [
    "produce_reports(data_augmented['label'], preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
